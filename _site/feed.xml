<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.2">Jekyll</generator><link href="https://ranyishere.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://ranyishere.github.io//" rel="alternate" type="text/html" /><updated>2017-07-28T15:18:08-07:00</updated><id>https://ranyishere.github.io//</id><title>Rany Tith</title><subtitle>Energy, Material Science, AI</subtitle><entry><title>Graphical Models, Exponential Families, and Variational Inferences</title><link href="https://ranyishere.github.io//probabilistic-graphical-models/" rel="alternate" type="text/html" title="Graphical Models, Exponential Families, and Variational Inferences" /><published>2017-07-28T00:00:00-07:00</published><updated>2017-07-28T00:00:00-07:00</updated><id>https://ranyishere.github.io//probabilistic-graphical-models</id><content type="html" xml:base="https://ranyishere.github.io//probabilistic-graphical-models/">&lt;p&gt;[ \frac{3}/{2} ]
# Generative Adversarial Networks&lt;/p&gt;

&lt;p&gt;This is a real interesting talk by Ian Goodfellow. He gives an overview applications of
a GAN and some of underlying theory. He makes a reference to a game theory approach.
This is because in a GAN model you have two players essentially. A discriminator and
a generator that have their own policies. A discriminator attempts to find counterfeits
that are produced by the generator, while the generator attempts to fool the discriminator. Through game theory, we are able to find out because of the strategies
that each player takes, the generator will always win in the end.&lt;/p&gt;

&lt;p&gt;Ian brings up an interesting point that the discriminator should be
a little stronger than the generator. He states it’s because the discriminator dictates what the generator learns. Furthermore, he mentions that the model has
issues when it comes to counting things in pictures.&lt;/p&gt;

&lt;p&gt;Possible uses of GAN:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simulated environments and training data&lt;/li&gt;
  &lt;li&gt;Stimulate missing data
    &lt;ul&gt;
      &lt;li&gt;Semi-supervised learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiple correct answers (more on this)&lt;/li&gt;
  &lt;li&gt;Realistic generation tasks&lt;/li&gt;
  &lt;li&gt;Simulation by prediction&lt;/li&gt;
  &lt;li&gt;Solve inference problems&lt;/li&gt;
  &lt;li&gt;Learn useful embeddings&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multiple-correct-answers&quot;&gt;Multiple correct answers&lt;/h2&gt;
&lt;p&gt;What Ian meant by having multiple correct answers is that your lists of possible
outcomes allowed from your model is more than one. For example, if you are trying
to predict cats, dogs, and human. We might not be able to distinguish that a given input is either one, but we are able to say that it is possible that the input is
one of the things in the list. Therefore, the input is accepted.&lt;/p&gt;

&lt;h2 id=&quot;learn-useful-embeddings&quot;&gt;Learn useful embeddings&lt;/h2&gt;
&lt;p&gt;There are tools like word2vec that are able to turn words into a vector in an embedded space. You are able to do the same thing with GANs but with pictures. For example, given a picture of a man with glasses, subtract a picture of a man, and a picture of a woman with glasses is returned.&lt;/p&gt;

&lt;h3 id=&quot;link-to-video&quot;&gt;Link to video:&lt;/h3&gt;

&lt;p&gt;http://on-demand.gputechconf.com/gtc/2017/video/s7502-ian-goodfellow-generative-adversarial-networks.mp4&lt;/p&gt;</content><category term="machine learning" /><category term="models" /><category term="video" /><category term="/r/machinelearning" /><summary>Paper from Foundations and Trends in Machine Learning</summary></entry><entry><title>Generative Adversarial Networks (GAN)</title><link href="https://ranyishere.github.io//gan-talk/" rel="alternate" type="text/html" title="Generative Adversarial Networks (GAN)" /><published>2017-07-28T00:00:00-07:00</published><updated>2017-07-28T00:00:00-07:00</updated><id>https://ranyishere.github.io//gan-talk</id><content type="html" xml:base="https://ranyishere.github.io//gan-talk/">&lt;p&gt;[ \frac{3}/{2} ]
# Generative Adversarial Networks&lt;/p&gt;

&lt;p&gt;This is a real interesting talk by Ian Goodfellow. He gives an overview applications of
a GAN and some underlying theory. He makes a reference to a game theory approach.
This is because in a GAN model you have two players essentially. A discriminator and
a generator that have their own policies. A discriminator attempts to find counterfeits
that are produced by the generator, while the generator attempts to fool the discriminator. Through game theory, we are able to find out because of the strategies
that each player takes, the generator will always win in the end.&lt;/p&gt;

&lt;p&gt;Ian brings up an interesting point that the discriminator should be
a little stronger than the generator. He states it’s because the discriminator dictates what the generator learns. Furthermore, he mentions that the model has
issues when it comes to counting things in pictures.&lt;/p&gt;

&lt;p&gt;Possible uses of GAN:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simulated environments and training data&lt;/li&gt;
  &lt;li&gt;Stimulate missing data
    &lt;ul&gt;
      &lt;li&gt;Semi-supervised learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiple correct answers (more on this)&lt;/li&gt;
  &lt;li&gt;Realistic generation tasks&lt;/li&gt;
  &lt;li&gt;Simulation by prediction&lt;/li&gt;
  &lt;li&gt;Solve inference problems&lt;/li&gt;
  &lt;li&gt;Learn useful embeddings&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multiple-correct-answers&quot;&gt;Multiple correct answers&lt;/h2&gt;
&lt;p&gt;What Ian meant by having multiple correct answers is that your lists of possible
outcomes allowed from your model is more than one. For example, if you are trying
to predict cats, dogs, and human. We might not be able to distinguish that a given input is either one, but we are able to say that it is possible that the input is
one of the things in the list. Therefore, the input is accepted.&lt;/p&gt;

&lt;h2 id=&quot;learn-useful-embeddings&quot;&gt;Learn useful embeddings&lt;/h2&gt;
&lt;p&gt;There are tools like word2vec that are able to turn words into a vector in an embedded space. You are able to do the same thing with GANs but with pictures. For example, given a picture of a man with glasses, subtract a picture of a man, and a picture of a woman with glasses is returned.&lt;/p&gt;

&lt;h3 id=&quot;link-to-video&quot;&gt;Link to video:&lt;/h3&gt;

&lt;p&gt;http://on-demand.gputechconf.com/gtc/2017/video/s7502-ian-goodfellow-generative-adversarial-networks.mp4&lt;/p&gt;</content><category term="machine learning" /><category term="models" /><category term="video" /><summary>Ian Goodfellow talk at GPU 2017</summary></entry><entry><title>Bayesian Optimization</title><link href="https://ranyishere.github.io//bayesian-optimization/" rel="alternate" type="text/html" title="Bayesian Optimization" /><published>2017-07-28T00:00:00-07:00</published><updated>2017-07-28T00:00:00-07:00</updated><id>https://ranyishere.github.io//bayesian-optimization</id><content type="html" xml:base="https://ranyishere.github.io//bayesian-optimization/">&lt;p&gt;[ \frac{3}{2} ]&lt;/p&gt;

&lt;h1 id=&quot;practical-bayesian-optimization-of-machine-learning-algorithms&quot;&gt;Practical Bayesian Optimization of Machine Learning Algorithms&lt;/h1&gt;
&lt;p&gt;## Jasper Snoek, Hugo Larochelle, and Ryan P. Adams; Harvard University&lt;/p&gt;

&lt;h3 id=&quot;link-to-paper&quot;&gt;Link to paper:&lt;/h3&gt;
&lt;p&gt;https://arxiv.org/pdf/1206.2944.pdf&lt;/p&gt;</content><category term="machine learning" /><category term="optimization" /><category term="paper" /><category term="statistics/probability" /><summary>Hyperspace parameter optimizaiton</summary></entry></feed>
